Timer unit: 1e-06 s

Total time: 526.522 s
File: test_ppo_overhead.py
Function: learn at line 31

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    31                                           @profile
    32                                           def learn(*, network, env, total_timesteps, eval_env = None, seed=None, nsteps=2048, ent_coef=0.0, lr=3e-4,
    33                                                       vf_coef=0.5,  max_grad_norm=0.5, gamma=0.99, lam=0.95,
    34                                                       log_interval=10, nminibatches=4, noptepochs=4, cliprange=0.2,
    35                                                       save_interval=0, load_path=None, model_fn=None, update_fn=None, init_fn=None, mpi_rank_weight=1, comm=None, **network_kwargs):
    36                                               '''
    37                                               Learn policy using PPO algorithm (https://arxiv.org/abs/1707.06347)
    38                                               Parameters:
    39                                               ----------
    40                                               network:                          policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)
    41                                                                                 specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns
    42                                                                                 tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward
    43                                                                                 neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.
    44                                                                                 See common/models.py/lstm for more details on using recurrent nets in policies
    45                                               env: baselines.common.vec_env.VecEnv     environment. Needs to be vectorized for parallel environment simulation.
    46                                                                                 The environments produced by gym.make can be wrapped using baselines.common.vec_env.DummyVecEnv class.
    47                                               nsteps: int                       number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where
    48                                                                                 nenv is number of environment copies simulated in parallel)
    49                                               total_timesteps: int              number of timesteps (i.e. number of actions taken in the environment)
    50                                               ent_coef: float                   policy entropy coefficient in the optimization objective
    51                                               lr: float or function             learning rate, constant or a schedule function [0,1] -> R+ where 1 is beginning of the
    52                                                                                 training and 0 is the end of the training.
    53                                               vf_coef: float                    value function loss coefficient in the optimization objective
    54                                               max_grad_norm: float or None      gradient norm clipping coefficient
    55                                               gamma: float                      discounting factor
    56                                               lam: float                        advantage estimation discounting factor (lambda in the paper)
    57                                               log_interval: int                 number of timesteps between logging events
    58                                               nminibatches: int                 number of training minibatches per update. For recurrent policies,
    59                                                                                 should be smaller or equal than number of environments run in parallel.
    60                                               noptepochs: int                   number of training epochs per update
    61                                               cliprange: float or function      clipping range, constant or schedule function [0,1] -> R+ where 1 is beginning of the training
    62                                                                                 and 0 is the end of the training
    63                                               save_interval: int                number of timesteps between saving events
    64                                               load_path: str                    path to load the model from
    65                                               **network_kwargs:                 keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network
    66                                                                                 For instance, 'mlp' network architecture has arguments num_hidden and num_layers.
    67                                               '''
    68                                           
    69         1       2583.0   2583.0      0.0      set_global_seeds(seed)
    70                                           
    71         1         13.0     13.0      0.0      if isinstance(lr, float): lr = constfn(lr)
    72                                               else: assert callable(lr)
    73         1          8.0      8.0      0.0      if isinstance(cliprange, float): cliprange = constfn(cliprange)
    74                                               else: assert callable(cliprange)
    75         1          7.0      7.0      0.0      total_timesteps = int(total_timesteps)
    76                                           
    77         1         53.0     53.0      0.0      policy = build_policy(env, network, **network_kwargs)
    78                                           
    79                                               # Get the nb of env
    80         1          8.0      8.0      0.0      nenvs = env.num_envs
    81                                           
    82                                               # Get state_space and action_space
    83         1          7.0      7.0      0.0      ob_space = env.observation_space
    84         1          6.0      6.0      0.0      ac_space = env.action_space
    85                                           
    86                                               # Calculate the batch_size
    87         1          6.0      6.0      0.0      nbatch = nenvs * nsteps
    88         1          6.0      6.0      0.0      nbatch_train = nbatch // nminibatches
    89         1         17.0     17.0      0.0      is_mpi_root = (MPI is None or MPI.COMM_WORLD.Get_rank() == 0)
    90                                           
    91                                               # Instantiate the model object (that creates act_model and train_model)
    92         1          6.0      6.0      0.0      if model_fn is None:
    93         1     188431.0 188431.0      0.0          from baselines.ppo2.model import Model
    94         1          6.0      6.0      0.0          model_fn = Model
    95                                           
    96         1          5.0      5.0      0.0      model = model_fn(policy=policy, ob_space=ob_space, ac_space=ac_space, nbatch_act=nenvs, nbatch_train=nbatch_train,
    97         1          4.0      4.0      0.0                      nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,
    98         1    3593966.0 3593966.0      0.7                      max_grad_norm=max_grad_norm, comm=comm, mpi_rank_weight=mpi_rank_weight)
    99                                           
   100         1          8.0      8.0      0.0      if load_path is not None:
   101                                                   model.load(load_path)
   102                                               # Instantiate the runner object
   103         1    1559675.0 1559675.0      0.3      runner = Runner(env=env, model=model, nsteps=nsteps, gamma=gamma, lam=lam)
   104         1          7.0      7.0      0.0      if eval_env is not None:
   105                                                   eval_runner = Runner(env = eval_env, model = model, nsteps = nsteps, gamma = gamma, lam= lam)
   106                                           
   107         1         12.0     12.0      0.0      epinfobuf = deque(maxlen=100)
   108         1          5.0      5.0      0.0      if eval_env is not None:
   109                                                   eval_epinfobuf = deque(maxlen=100)
   110                                           
   111         1          8.0      8.0      0.0      if init_fn is not None:
   112                                                   init_fn()
   113                                           
   114                                               # Start total timer
   115         1         12.0     12.0      0.0      tfirststart = time.perf_counter()
   116                                           
   117         1          5.0      5.0      0.0      nupdates = total_timesteps//nbatch
   118         8         39.0      4.9      0.0      for update in range(1, nupdates+1):
   119         7         33.0      4.7      0.0          assert nbatch % nminibatches == 0
   120                                                   # Start timer
   121         7         41.0      5.9      0.0          tstart = time.perf_counter()
   122         7         60.0      8.6      0.0          frac = 1.0 - (update - 1.0) / nupdates
   123                                                   # Calculate the learning rate
   124         7         48.0      6.9      0.0          lrnow = lr(frac)
   125                                                   # Calculate the cliprange
   126         7         86.0     12.3      0.0          cliprangenow = cliprange(frac)
   127                                           
   128         7         35.0      5.0      0.0          if update % log_interval == 0 and is_mpi_root: logger.info('Stepping environment...')
   129                                           
   130                                                   # Get minibatch
   131         7  520648815.0 74378402.1     98.9          obs, returns, masks, actions, values, neglogpacs, states, epinfos = runner.run() #pylint: disable=E0632
   132         7         52.0      7.4      0.0          if eval_env is not None:
   133                                                       eval_obs, eval_returns, eval_masks, eval_actions, eval_values, eval_neglogpacs, eval_states, eval_epinfos = eval_runner.run() #pylint: disable=E0632
   134                                           
   135         7         41.0      5.9      0.0          if update % log_interval == 0 and is_mpi_root: logger.info('Done.')
   136                                           
   137         7        140.0     20.0      0.0          epinfobuf.extend(epinfos)
   138         7         36.0      5.1      0.0          if eval_env is not None:
   139                                                       eval_epinfobuf.extend(eval_epinfos)
   140                                           
   141                                                   # Here what we're going to do is for each minibatch calculate the loss and append it.
   142         7        136.0     19.4      0.0          mblossvals = []
   143         7         35.0      5.0      0.0          if states is None: # nonrecurrent version
   144                                                       # Index of each element of batch_size
   145                                                       # Create the indices array
   146         7        138.0     19.7      0.0              inds = np.arange(nbatch)
   147        35        215.0      6.1      0.0              for _ in range(noptepochs):
   148                                                           # Randomize the indexes
   149        28        874.0     31.2      0.0                  np.random.shuffle(inds)
   150                                                           # 0 to batch_size with batch_train_size step
   151       140        959.0      6.8      0.0                  for start in range(0, nbatch, nbatch_train):
   152       112        587.0      5.2      0.0                      end = start + nbatch_train
   153       112        940.0      8.4      0.0                      mbinds = inds[start:end]
   154       112        849.0      7.6      0.0                      slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))
   155       112     519736.0   4640.5      0.1                      mblossvals.append(model.train(lrnow, cliprangenow, *slices))
   156                                                   else: # recurrent version
   157                                                       assert nenvs % nminibatches == 0
   158                                                       envsperbatch = nenvs // nminibatches
   159                                                       envinds = np.arange(nenvs)
   160                                                       flatinds = np.arange(nenvs * nsteps).reshape(nenvs, nsteps)
   161                                                       for _ in range(noptepochs):
   162                                                           np.random.shuffle(envinds)
   163                                                           for start in range(0, nenvs, envsperbatch):
   164                                                               end = start + envsperbatch
   165                                                               mbenvinds = envinds[start:end]
   166                                                               mbflatinds = flatinds[mbenvinds].ravel()
   167                                                               slices = (arr[mbflatinds] for arr in (obs, returns, masks, actions, values, neglogpacs))
   168                                                               mbstates = states[mbenvinds]
   169                                                               mblossvals.append(model.train(lrnow, cliprangenow, *slices, mbstates))
   170                                           
   171                                                   # Feedforward --> get losses --> update
   172         7       1190.0    170.0      0.0          lossvals = np.mean(mblossvals, axis=0)
   173                                                   # End timer
   174         7         55.0      7.9      0.0          tnow = time.perf_counter()
   175                                                   # Calculate the fps (frame per second)
   176         7         57.0      8.1      0.0          fps = int(nbatch / (tnow - tstart))
   177                                           
   178         7         34.0      4.9      0.0          if update_fn is not None:
   179                                                       update_fn(update)
   180                                           
   181         7         39.0      5.6      0.0          if update % log_interval == 0 or update == 1:
   182                                                       # Calculates if value function is a good predicator of the returns (ev > 1)
   183                                                       # or if it's just worse than predicting nothing (ev =< 0)
   184         1        352.0    352.0      0.0              ev = explained_variance(values, returns)
   185         1         26.0     26.0      0.0              logger.logkv("misc/serial_timesteps", update*nsteps)
   186         1          9.0      9.0      0.0              logger.logkv("misc/nupdates", update)
   187         1         31.0     31.0      0.0              logger.logkv("misc/total_timesteps", update*nbatch)
   188         1          9.0      9.0      0.0              logger.logkv("fps", fps)
   189         1         10.0     10.0      0.0              logger.logkv("misc/explained_variance", float(ev))
   190         1        141.0    141.0      0.0              logger.logkv('eprewmean', safemean([epinfo['r'] for epinfo in epinfobuf]))
   191         1        111.0    111.0      0.0              logger.logkv('eplenmean', safemean([epinfo['l'] for epinfo in epinfobuf]))
   192         1         26.0     26.0      0.0              if eval_env is not None:
   193                                                           logger.logkv('eval_eprewmean', safemean([epinfo['r'] for epinfo in eval_epinfobuf]) )
   194                                                           logger.logkv('eval_eplenmean', safemean([epinfo['l'] for epinfo in eval_epinfobuf]) )
   195         1         11.0     11.0      0.0              logger.logkv('misc/time_elapsed', tnow - tfirststart)
   196         6         64.0     10.7      0.0              for (lossval, lossname) in zip(lossvals, model.loss_names):
   197         5         48.0      9.6      0.0                  logger.logkv('loss/' + lossname, lossval)
   198                                           
   199         1       1035.0   1035.0      0.0              logger.dumpkvs()
   200         7         32.0      4.6      0.0          if save_interval and (update % save_interval == 0 or update == 1) and logger.get_dir() and is_mpi_root:
   201                                                       checkdir = osp.join(logger.get_dir(), 'checkpoints')
   202                                                       os.makedirs(checkdir, exist_ok=True)
   203                                                       savepath = osp.join(checkdir, '%.5i'%update)
   204                                                       print('Saving to', savepath)
   205                                                       model.save(savepath)
   206                                           
   207         1          3.0      3.0      0.0      return model

