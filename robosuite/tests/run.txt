Timer unit: 1e-06 s

Total time: 281.485 s
File: test_ppo_overhead.py
Function: run at line 46

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    46                                               @profile
    47                                               def run(self):
    48                                                   # Here, we init the lists that will contain the mb of experiences
    49         3         14.0      4.7      0.0          mb_obs, mb_rewards, mb_actions, mb_values, mb_dones, mb_neglogpacs = [],[],[],[],[],[]
    50         3          9.0      3.0      0.0          mb_states = self.states
    51         3          5.0      1.7      0.0          epinfos = []
    52                                                   # For n in range number of steps
    53       387       1060.0      2.7      0.0          for _ in range(self.nsteps):
    54                                                       # Given observations, get action value and neglopacs
    55                                                       # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init
    56       384    1152165.0   3000.4      0.4              actions, values, self.states, neglogpacs = self.model.step(self.obs, S=self.states, M=self.dones)
    57       384       6078.0     15.8      0.0              mb_obs.append(self.obs.copy())
    58       384        969.0      2.5      0.0              mb_actions.append(actions)
    59       384        829.0      2.2      0.0              mb_values.append(values)
    60       384        918.0      2.4      0.0              mb_neglogpacs.append(neglogpacs)
    61       384        975.0      2.5      0.0              mb_dones.append(self.dones)
    62                                           
    63                                                       # Take actions in env and look the results
    64                                                       # Infos contains a ton of useful informations
    65       384  280267660.0 729863.7     99.6              self.obs[:], rewards, self.dones, infos = self.env.step(actions)
    66       768       2408.0      3.1      0.0              for info in infos:
    67       384       1915.0      5.0      0.0                  maybeepinfo = info.get('episode')
    68       384        715.0      1.9      0.0                  if maybeepinfo: epinfos.append(maybeepinfo)
    69       384        910.0      2.4      0.0              mb_rewards.append(rewards)
    70                                                   #batch of steps to batch of rollouts
    71         3        705.0    235.0      0.0          mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)
    72         3        652.0    217.3      0.0          mb_rewards = np.asarray(mb_rewards, dtype=np.float32)
    73         3        853.0    284.3      0.0          mb_actions = np.asarray(mb_actions)
    74         3        729.0    243.0      0.0          mb_values = np.asarray(mb_values, dtype=np.float32)
    75         3        779.0    259.7      0.0          mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)
    76         3        495.0    165.0      0.0          mb_dones = np.asarray(mb_dones, dtype=np.bool)
    77         3      25712.0   8570.7      0.0          last_values = self.model.value(self.obs, S=self.states, M=self.dones)
    78                                           
    79                                                   # discount/bootstrap off value fn
    80         3        198.0     66.0      0.0          mb_returns = np.zeros_like(mb_rewards)
    81         3         85.0     28.3      0.0          mb_advs = np.zeros_like(mb_rewards)
    82         3          6.0      2.0      0.0          lastgaelam = 0
    83       387        797.0      2.1      0.0          for t in reversed(range(self.nsteps)):
    84       384        925.0      2.4      0.0              if t == self.nsteps - 1:
    85         3         60.0     20.0      0.0                  nextnonterminal = 1.0 - self.dones
    86         3          6.0      2.0      0.0                  nextvalues = last_values
    87                                                       else:
    88       381       3181.0      8.3      0.0                  nextnonterminal = 1.0 - mb_dones[t+1]
    89       381       1089.0      2.9      0.0                  nextvalues = mb_values[t+1]
    90       384       7251.0     18.9      0.0              delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]
    91       384       4494.0     11.7      0.0              mb_advs[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam
    92         3         22.0      7.3      0.0          mb_returns = mb_advs + mb_values
    93         3         17.0      5.7      0.0          return (*map(sf01, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs)),
    94         3        179.0     59.7      0.0              mb_states, epinfos)

